{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "sql_datastore = Datastore.get(workspace=ws, datastore_name=\"ado_sql_datastore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "query = DataPath(sql_datastore, 'SELECT *  FROM Improvements')\n",
    "improvements_sql_ds = Dataset.Tabular.from_sql_query(query)\n",
    "\n",
    "improvements_sql_ds.register(workspace=ws,\n",
    "                          name=\"ai_ag_ado_improvements\",\n",
    "                          description = \"Improvements from Azure DevOps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "query_string = 'SELECT *, (POWER(1.5,MitigationScore) * POWER(2,Priority) * POWER(6.585, IsBlocker)) as dc_impact_score FROM FeedbackItems'\n",
    "\n",
    "query = DataPath(sql_datastore, query_string)\n",
    "feedback_sql_ds = Dataset.Tabular.from_sql_query(query)\n",
    "\n",
    "feedback_sql_ds.register(workspace=ws,\n",
    "                         name=\"ai_ag_ado_feedack\",\n",
    "                         description = \"Feedback from Azure DevOps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedback_sql_pd = feedback_sql_ds.to_pandas_dataframe()\n",
    "# \n",
    "# label =\"dc_impact_score\"\n",
    "# \n",
    "# def dc_impact_score_calculation(mitigation_score, priority, is_blocker):\n",
    "#     return (1.5**mitigation_score) * (2**priority) * (6.585**is_blocker)\n",
    "#     \n",
    "# \n",
    "# feedback_sql_pd[label] = dc_impact_score_calculation(feedback_sql_pd['MitigationScore'], feedback_sql_pd['Priority'], feedback_sql_pd['IsBlocker'])\n",
    "# \n",
    "# file=\"temp\"\n",
    "# feedback_sql_pd.to_csv(file)\n",
    "# feedback_sql_ds_labeled = Dataset.Tabular.from_delimited_files(path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    # Split the dataset into train and test datasets\n",
    "    train_data, test_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "\n",
    "    # Register the train dataset with your workspace\n",
    "    train_data.register(workspace = ws, \n",
    "                        name = 'ai_ag_ado_feedack_train_dataset',\n",
    "                        description = 'Feedback from Azure DevOps training data',\n",
    "                        create_new_version=True)\n",
    "\n",
    "    # Register the test dataset with your workspace\n",
    "    test_data.register(workspace = ws, \n",
    "                       name = 'ai_ag_ado_feedack_test_dataset', \n",
    "                       description = 'Feedback from Azure DevOps test data')\n",
    "    return train_data, test_data\n",
    "    \n",
    "train_data, test_data = split_dataset(feedback_sql_ds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "\n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 4)\n",
    "\n",
    "    # Create the cluster.\\n\",\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "\n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "\n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             compute_target=compute_target,\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             \"verbosity\": logging.INFO,\n",
    "                             \"enable_early_stopping\": True, \n",
    "                             \"experiment_timeout_minutes\" : 10,\n",
    "                             \"max_concurrent_iterations\": 4,\n",
    "                             \"max_cores_per_iteration\": -1,\n",
    "                             \"n_cross_validations\": 5,\n",
    "                             \"primary_metric\": 'normalized_root_mean_squared_error'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"ai-impact-score-experiment-dc-sql\")\n",
    "\n",
    "runs = experiment.get_runs()\n",
    "\n",
    "if not runs:\n",
    "    remote_run = experiment.submit(automl_config, show_output=True)\n",
    "else:\n",
    "    for run in runs:\n",
    "        remote_run = run\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(remote_run.get_children())\n",
    "metricslist = {}\n",
    "for run in children:\n",
    "    properties = run.get_properties()\n",
    "    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = fitted_model.predict(x_train)\n",
    "y_residual_train = y_train - y_pred_train\n",
    "\n",
    "y_pred_test = fitted_model.predict(x_test)\n",
    "y_residual_test = y_test - y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set up a multi-plot chart.\n",
    "f, (a0, a1) = plt.subplots(1, 2, gridspec_kw = {'width_ratios':[1, 1], 'wspace':0, 'hspace': 0})\n",
    "f.suptitle('Regression Residual Values', fontsize = 18)\n",
    "f.set_figheight(6)\n",
    "f.set_figwidth(16)\n",
    "\n",
    "# Plot residual values of training set.\n",
    "a0.axis([0, 360, -200, 200])\n",
    "a0.plot(y_residual_train, 'bo', alpha = 0.5)\n",
    "a0.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a0.text(16,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))), fontsize = 12)\n",
    "a0.text(16,140,'R2 score = {0:.2f}'.format(r2_score(y_train, y_pred_train)), fontsize = 12)\n",
    "a0.set_xlabel('Training samples', fontsize = 12)\n",
    "a0.set_ylabel('Residual Values', fontsize = 12)\n",
    "\n",
    "# Plot a histogram.\n",
    "a0.hist(y_residual_train, orientation = 'horizontal', color = 'b', bins = 10, histtype = 'step')\n",
    "a0.hist(y_residual_train, orientation = 'horizontal', color = 'b', alpha = 0.2, bins = 10)\n",
    "\n",
    "# Plot residual values of test set.\n",
    "a1.axis([0, 90, -200, 200])\n",
    "a1.plot(y_residual_test, 'bo', alpha = 0.5)\n",
    "a1.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a1.text(5,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))), fontsize = 12)\n",
    "a1.text(5,140,'R2 score = {0:.2f}'.format(r2_score(y_test, y_pred_test)), fontsize = 12)\n",
    "a1.set_xlabel('Test samples', fontsize = 12)\n",
    "a1.set_yticklabels([])\n",
    "\n",
    "# Plot a histogram.\n",
    "a1.hist(y_residual_test, orientation = 'horizontal', color = 'b', bins = 10, histtype = 'step')\n",
    "a1.hist(y_residual_test, orientation = 'horizontal', color = 'b', alpha = 0.2, bins = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='best_sql_dc_impact_score_model', model_path='./outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registered model:\\n --> Name: {}\\n --> Version: {}\\n --> URL: {}\".format(model.name, model.version, model.url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLHyperparameterTuning] *",
   "language": "python",
   "name": "conda-env-MLHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}