{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import azureml.dataprep\n",
    "from azureml.automl.core.constants import FeaturizationConfigMode\n",
    "print(azureml.dataprep.__version__)\n",
    "import azureml.core\n",
    "print(azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "sql_datastore = Datastore.get(workspace=ws, datastore_name=\"ado_sql_datastore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "label =\"MitigationScore\"\n",
    "query_string = 'SELECT MitigationDescription, MitigationScore FROM FeedbackItems'\n",
    "\n",
    "query = DataPath(sql_datastore, query_string)\n",
    "feedback_sql_ds = Dataset.Tabular.from_sql_query(query)\n",
    "\n",
    "feedback_sql_ds.register(workspace=ws,\n",
    "                         name=\"ai_ag_ado_feedack_mitigation_score\",\n",
    "                         description = \"Feedback from Azure DevOps\",\n",
    "                         create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    # Split the dataset into train and test datasets\n",
    "    train_data, test_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "\n",
    "    # Register the train dataset with your workspace\n",
    "    train_data.register(workspace = ws, \n",
    "                        name = 'ai_ag_ado_feedack_mitigation_score_train_dataset',\n",
    "                        description = 'Feedback from Azure DevOps training data',\n",
    "                        create_new_version=True)\n",
    "\n",
    "    # Register the test dataset with your workspace\n",
    "    test_data.register(workspace = ws, \n",
    "                       name = 'ai_ag_ado_feedack_mitigation_score_test_dataset', \n",
    "                       description = 'Feedback from Azure DevOps test data',\n",
    "                       create_new_version=True)\n",
    "    return train_data, test_data\n",
    "    \n",
    "train_data, test_data = split_dataset(feedback_sql_ds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             verbosity = logging.INFO,\n",
    "                             enable_early_stopping = True,\n",
    "                             max_concurrent_iterations = 4,\n",
    "                             max_cores_per_iteration = -1,\n",
    "                             n_cross_validations = 5,\n",
    "                             primary_metric ='r2_score',\n",
    "                             preprocess=True,\n",
    "                             featurization = FeaturizationConfigMode.Auto,\n",
    "                             enable_tf = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"mitigation-score-dc-sql\")\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "\n",
    "model = best_run.register_model(model_name='best_sql_dc_mitigation_score', model_path='./outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registered model:\\n --> Name: {}\\n --> Version: {}\\n --> URL: {}\".format(model.name, model.version, model.url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLHyperparameterTuning] *",
   "language": "python",
   "name": "conda-env-MLHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}